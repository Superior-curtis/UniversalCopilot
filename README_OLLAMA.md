# Universal Copilot - Phase A (Windows) with Ollama

**Free, private, offline inline AI suggestions for Windows**

Type in Notepad, see AI suggestions in real-time. Press TAB to accept, ESC to dismiss. **Everything runs on your machine.**

## âš¡ Quick Start

1. **Download** the latest release
2. **Run** `install.bat`
3. **Follow the prompts** (downloads Ollama + Mistral model)
4. **Type in Notepad** - suggestions appear automatically!

## ðŸŽ¯ Features

- âœ… **Zero cost** - no API keys, no subscriptions
- âœ… **Completely private** - all processing local, no cloud telemetry
- âœ… **Works offline** - internet not required
- âœ… **Fast** - suggestions in <300ms thanks to local inference
- âœ… **System-wide** - works with Notepad, Google Docs web, Word, etc.
- âœ… **Native Windows** - layered overlay, low-level keyboard hooks
- âœ… **Open source** - Rust + Ollama + Mistral-7B

## ðŸ“‹ System Requirements

- **Windows 10+** (x64)
- **8GB+ RAM** (for Mistral-7B model in VRAM)
- **5GB disk space** (model download)
- **Ollama** (auto-installed via `install.bat`)

## ðŸ”§ How It Works

```
User typing in Notepad
    â†“
UniversalCopilot detects caret position and captures surrounding text
    â†“
Sends context to local Ollama server (localhost:11434)
    â†“
Mistral-7B generates 1-3 word completion in <300ms
    â†“
Ghost text overlay appears above cursor
    â†“
User presses TAB (accept) or ESC (dismiss)
    â†“
If accepted, suggestion is injected into the document
```

## ðŸ“š Architecture

### Win32 Layer (C interface via `windows` crate)
- **Layered window rendering** for transparent ghost-text overlay
- **Low-level keyboard hook** (`WH_KEYBOARD_LL`) for TAB/ESC detection
- **Caret position tracking** via `GetGUIThreadInfo`, `GetCaretPos`
- **Text capture** via `WM_GETTEXT` and `EM_GETSEL`
- **Input simulation** via `SendInput` for TAB injection

### Async Runtime (Tokio)
- Parallel polling of caret position (300ms debounce)
- Non-blocking inference requests to Ollama
- Cancellation signaling for rapid re-prediction on user edits

### Networking
- **HTTP POST to Ollama API** - no external dependencies
- **Streaming-ready** architecture for future token-by-token updates

## ðŸš€ Usage

### Basic Operation
1. Open any Windows application (Notepad, Word, etc.)
2. Start typing
3. Suggestions appear as light text overlaid above your cursor
4. Press **TAB** to accept the suggestion (injects text)
5. Press **ESC** to dismiss

### Customization
- **Model**: Change in `src/llm.rs` (line ~7) - try `llama2`, `neural-chat`, etc.
  ```bash
  ollama pull llama2
  # Then edit MODEL = "llama2:latest"
  ```
- **Temperature**: Adjust creativity vs focus in `src/llm.rs`
- **Max tokens**: Tune suggestion length

## ðŸ“– Documentation

- [OLLAMA_SETUP.md](OLLAMA_SETUP.md) - Detailed Ollama setup guide
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Common issues & fixes (coming soon)
- [ARCHITECTURE.md](ARCHITECTURE.md) - Deep dive into the codebase (coming soon)

## ðŸ› ï¸ Building from Source

```bash
# Prerequisites: Rust 1.70+, Windows 10+ x64
cargo build --release

# Binary: target\release\universal_copilot.exe
```

## ðŸ“‹ Technical Stack

- **Language**: Rust (edition 2021)
- **Async Runtime**: Tokio 1.35
- **Win32 API**: windows-rs 0.48
- **HTTP Client**: reqwest 0.11 (async)
- **UI**: Native Win32 layered windows + GDI+
- **LLM Engine**: Ollama (local inference server)
- **Models**: Mistral-7B (default), any GGUF model supported

## ðŸŽ“ Learning Resources

This project demonstrates:
- Win32 API usage in Rust (keyboard hooks, window creation, GDI rendering)
- Async Rust with Tokio (multi-threaded runtime, channels, watches)
- HTTP client implementation with streaming support
- Low-latency desktop application development

## ðŸ¤ Contributing

Contributions welcome! Areas for enhancement:
- Browser extensions for Chrome/Firefox
- Web editor support (Google Docs, GitHub, etc.)
- Additional model backends (local ONNX, TensorRT)
- Performance optimizations (GPU acceleration)
- More languages/locales

## âš–ï¸ License

MIT License - See LICENSE file

## â“ FAQ

**Q: Why Ollama instead of OpenAI API?**
A: Privacy, cost, offline capability. Ollama gives you a free local LLM server that's completely private and works without internet.

**Q: Will it work with web editors like Google Docs?**
A: Not yet - those use browser DOM text which Win32 APIs can't access. This is a planned Phase B feature (browser extension approach).

**Q: How much VRAM does Mistral need?**
A: ~7-8GB. Quantized to Q4 for optimal speed/quality tradeoff.

**Q: Can I use a different model?**
A: Yes! Any GGUF model works. Try `ollama pull neural-chat` or `ollama pull llama2`.

**Q: Is it safe to use?**
A: Yes. All inference happens locally on your machine. No data is sent to cloud servers. No telemetry.

## ðŸŽ‰ Status

**Phase A Complete:**
- âœ… Local LLM integration (Ollama)
- âœ… Caret detection in Notepad
- âœ… Ghost-text overlay rendering
- âœ… TAB/ESC keyboard handling
- âœ… One-click installer
- âœ… Fully open source

**Phase B (Planned):**
- ðŸ”„ Browser extensions (Chrome, Firefox)
- ðŸ”„ Web editor support (Google Docs, GitHub)
- ðŸ”„ Multi-language support
- ðŸ”„ Advanced customization UI
- ðŸ”„ Performance optimizations

---

**Made with â¤ï¸ using Rust + Ollama**

Follow the [Quick Start](#quick-start) guide above to get started!
